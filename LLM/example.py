import os
from dotenv import load_dotenv
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import FAISS
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ë²¡í„° DB ë¡œë“œ
db_path = "../Data/vector_db_openai_large_combined"

embedding_model = OpenAIEmbeddings(model="text-embedding-3-large")
vector_db = FAISS.load_local(
    folder_path=db_path,
    embeddings=embedding_model,
    allow_dangerous_deserialization=True
)

# LLM ì´ˆê¸°í™” (GPT-4 ë˜ëŠ” GPT-3.5 ê°€ëŠ¥)
llm = ChatOpenAI(model="gpt-4o", temperature=0)

# ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì •ì˜
system_prompt = (
    "ë‹¹ì‹ ì€ í•œêµ­ì˜ ì²­ë…„ ì •ì±…ì— ëŒ€í•œ ì§ˆë¬¸-ë‹µë³€ ë„ìš°ë¯¸ì…ë‹ˆë‹¤. "
    "ê²€ìƒ‰ëœ ë‹¤ìŒ ì •ë³´ë“¤ì„ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”. "
    "ë‹µì„ ëª¨ë¥´ë©´ ëª¨ë¥¸ë‹¤ê³  ë§í•˜ì„¸ìš”. "
    "\n\n"
    "{context}"
)

# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±
qa_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system_prompt),
        ("human", "{input}"),
    ]
)

# ë¦¬íŠ¸ë¦¬ë²„ ìƒì„± (ë” ë§ì€ ë¬¸ì„œ ê²€ìƒ‰ì„ ìœ„í•´ k ê°’ ì¦ê°€)
retriever = vector_db.as_retriever(search_kwargs={"k": 5})

# ë¬¸ì„œ ê²°í•© ì²´ì¸ ìƒì„±
question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)

# ìµœì¢… RAG ì²´ì¸ ìƒì„± (ìµœì‹  API ì‚¬ìš©)
rag_chain = create_retrieval_chain(retriever, question_answer_chain)

# ì‚¬ìš©ì ì§ˆë¬¸
user_question = "ì„œìš¸ì— ì‚¬ëŠ” 26ì‚´ ì²­ë…„ì¸ë° ì£¼ê±°ë¹„ë¥¼ ì§€ì›í•´ì£¼ëŠ” ì •ì±…ì´ ë­ê°€ ìˆì„ê¹Œ?"

# ì§ˆì˜ ì‘ë‹µ ìˆ˜í–‰
result = rag_chain.invoke({"input": user_question})

# ì¶œë ¥
print("ğŸ§‘ ì‚¬ìš©ì ì§ˆë¬¸:")
print(user_question)
print("\nğŸ¤– ì±—ë´‡ ë‹µë³€:")
print(result['answer'])

print("\nğŸ“š ê´€ë ¨ ì •ì±… ë¬¸ì„œ ì œëª©:")
for doc in result['context']:
    print("-", doc.metadata.get("ì •ì±…ëª…", "ì œëª© ì—†ìŒ"))
